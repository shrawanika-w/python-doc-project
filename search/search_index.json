{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RAG Document Search AI","text":""},{"location":"#overview","title":"Overview","text":"<p>This repository contains a Retrieval Augmented Generation (RAG) based document search platform designed for enterprise use. The system enables semantic search over structured and unstructured documents using embeddings, vector search, and large language models.</p> <p>The platform is built with operability, security, and scalability in mind and follows standard architectural and governance practices expected in regulated environments.</p>"},{"location":"#what-this-system-does","title":"What This System Does","text":"<ul> <li>Ingests enterprise documents</li> <li>Converts content into embeddings</li> <li>Stores vectors in a scalable vector database</li> <li>Retrieves relevant context for user queries</li> <li>Generates grounded responses using an LLM</li> <li>Exposes APIs for integration with applications and chat interfaces</li> </ul>"},{"location":"#intended-audience","title":"Intended Audience","text":"<p>This documentation is intended for: - Software Engineers - Architects - SRE and DevOps teams - AI and ML platform teams - Security and Risk reviewers - Product and Business stakeholders</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#architecture","title":"Architecture","text":"<p>High-level and detailed architecture views following the C4 Model: - System overview - Context diagram - Container diagram - Component diagram</p>"},{"location":"#adrs","title":"ADRs","text":"<p>Architecture Decision Records explaining why key technical choices were made: - Framework selection - LLM provider - Vector database</p>"},{"location":"#api","title":"API","text":"<p>OpenAPI specification for all public endpoints.</p>"},{"location":"#runbooks","title":"Runbooks","text":"<p>Operational guides for: - Production support - Incident handling - Disaster recovery</p>"},{"location":"#key-technology-choices","title":"Key Technology Choices","text":"<ul> <li>API Framework: FastAPI</li> <li>LLM Provider: Gemini</li> <li>Vector Store: PostgreSQL with pgvector</li> <li>Observability: Prometheus, Grafana</li> <li>Documentation: Markdown, Mermaid, MkDocs</li> </ul>"},{"location":"#non-goals","title":"Non-Goals","text":"<p>This system is not intended to: - Train foundation models - Replace enterprise search engines - Operate without human oversight in regulated workflows</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Review the Architecture section for system design</li> <li>Read ADRs to understand key decisions</li> <li>Refer to API docs for integration</li> <li>Use Runbooks for operational readiness</li> </ol>"},{"location":"#governance-and-compliance","title":"Governance and Compliance","text":"<p>The platform aligns with: - Secure SDLC practices - Model risk management principles - Audit and traceability requirements - Data minimization and access control standards</p>"},{"location":"#references","title":"References","text":"<ul> <li>Architecture: <code>/architecture/system-overview.md</code></li> <li>ADRs: <code>/adr</code></li> <li>API Spec: <code>/api/openapi.yaml</code></li> <li>Production Runbook: <code>/runbooks/prod.md</code></li> </ul>"},{"location":"adr/001-use-fastapi/","title":"ADR 001: Use FastAPI","text":"<p>Status: Accepted Date: 2026-01-22  </p>"},{"location":"adr/001-use-fastapi/#context","title":"Context","text":"<p>We are building a RAG-based document search AI service that will: - Accept user queries from multiple clients - Interface with a vector database - Call a large language model for answers - Serve multiple concurrent requests efficiently</p>"},{"location":"adr/001-use-fastapi/#decision","title":"Decision","text":"<p>We will use FastAPI as the backend framework.</p>"},{"location":"adr/001-use-fastapi/#rationale","title":"Rationale","text":"<ol> <li>Performance: Supports async and high concurrency</li> <li>Developer Productivity: Type hints, dependency injection</li> <li>Auto-generated API docs (OpenAPI/Swagger)</li> <li>Easy testing &amp; CI/CD integration</li> <li>Strong community and ecosystem</li> </ol>"},{"location":"adr/001-use-fastapi/#consequences","title":"Consequences","text":"<ul> <li>All endpoints async</li> <li>Frontend can rely on auto-generated docs</li> <li>Security patching is mandatory</li> <li>Migration to other frameworks will require refactoring</li> </ul>"},{"location":"adr/002-llm-provider/","title":"ADR 002: LLM Provider","text":"<p>Status: Accepted Date: 2026-01-22  </p>"},{"location":"adr/002-llm-provider/#context","title":"Context","text":"<p>Our RAG-based document search AI service requires a large language model (LLM) to:</p> <ul> <li>Interpret natural language queries from users.</li> <li>Summarize and generate relevant answers based on retrieved documents.</li> <li>Support multiple languages and domain-specific terminology.</li> <li>Scale efficiently with low latency for multiple concurrent requests.</li> </ul> <p>We evaluated multiple LLM providers, including OpenAI GPT, Anthropic Claude, and Gemini.</p>"},{"location":"adr/002-llm-provider/#decision","title":"Decision","text":"<p>We will use Gemini LLM as the provider for our RAG service.</p>"},{"location":"adr/002-llm-provider/#rationale","title":"Rationale","text":"<ol> <li>Performance &amp; Accuracy: Gemini has high accuracy on reasoning and summarization tasks, particularly with financial documents.  </li> <li>Cost Efficiency: Competitive pricing for large-scale API usage compared to other providers.  </li> <li>Latency &amp; Reliability: Provides low-latency API endpoints suitable for interactive queries.  </li> <li>Security &amp; Compliance: Data handling and storage policies align with banking regulations.  </li> <li>Ecosystem &amp; Integrations: Provides SDKs and tools compatible with Python, making integration with FastAPI straightforward.  </li> <li>Future-proofing: Continuous updates from the provider ensure access to new capabilities and improvements.</li> </ol>"},{"location":"adr/002-llm-provider/#consequences","title":"Consequences","text":"<ul> <li>All query requests will route through Gemini APIs.  </li> <li>Backend service will handle token limits and implement caching for efficiency.  </li> <li>Team will monitor costs and usage to prevent overages.  </li> <li>Training custom models on Gemini may be considered in the future for domain-specific enhancements.  </li> <li>Switching to another LLM provider will require updating API calls and re-testing response behavior.</li> </ul>"},{"location":"adr/003-vector-db/","title":"ADR 003: Vector Database","text":"<p>Status: Accepted Date: 2026-01-22  </p>"},{"location":"adr/003-vector-db/#context","title":"Context","text":"<p>The RAG-based document search AI service needs a database to store documents and embeddings:</p> <ul> <li>Must handle structured and unstructured data efficiently.</li> <li>Support fast vector similarity search for embeddings.</li> <li>Ensure ACID compliance for transactional consistency.</li> <li>Integrate well with Python, FastAPI, and the overall service architecture.</li> </ul> <p>Several options were considered, including PostgreSQL, Chroma, and Pinecone.</p>"},{"location":"adr/003-vector-db/#decision","title":"Decision","text":"<p>We will use PostgreSQL as the primary database for storing documents, metadata, and vector embeddings.</p>"},{"location":"adr/003-vector-db/#rationale","title":"Rationale","text":"<ol> <li>Maturity &amp; Reliability: PostgreSQL is a proven, enterprise-grade relational database with strong consistency guarantees.  </li> <li>Vector Search Support: Native support for vector data types and indexing (via <code>pgvector</code> extension) allows efficient similarity search.  </li> <li>Integration: Excellent Python support via <code>psycopg2</code> / <code>SQLAlchemy</code>, making integration with FastAPI straightforward.  </li> <li>Security &amp; Compliance: Enterprise security, encryption, and auditing features align with banking standards.  </li> <li>Cost-Effective: Open-source, widely supported, and can scale vertically or horizontally with extensions.  </li> <li>Operational Simplicity: Teams already have PostgreSQL expertise, reducing onboarding and operational overhead.  </li> </ol>"},{"location":"adr/003-vector-db/#consequences","title":"Consequences","text":"<ul> <li>Use the <code>pgvector</code> extension to store embeddings efficiently.  </li> <li>Backend must implement batch writes and similarity queries optimized for RAG workflows.  </li> <li>Migration to another vector DB in the future would require changes in query logic and indexing.  </li> <li>PostgreSQL maintenance, backups, and scaling strategies must be defined and monitored.  </li> <li>Hybrid approach is possible: PostgreSQL for metadata + specialized vector DB if needed for extreme scale.  </li> </ul>"},{"location":"architecture/system-overview/","title":"System Overview","text":"<pre><code>flowchart LR\n    User[User / Client] --&gt;|Query| API[FastAPI Gateway]\n\n    API --&gt;|Request| RAG[RAG Worker]\n\n    RAG --&gt;|Generate Embedding| Embed[Embedding Model]\n    Embed --&gt; RAG\n\n    RAG --&gt;|Vector Search| VectorDB[(PostgreSQL&lt;br/&gt;pgvector)]\n    VectorDB --&gt;|Relevant Chunks| RAG\n\n    RAG --&gt;|Context + Prompt| LLM[Gemini LLM]\n    LLM --&gt;|Generated Answer| RAG\n\n    RAG --&gt;|Response| API\n    API --&gt;|Answer| User\n\n    Docs[(Object Storage&lt;br/&gt;Documents)] --&gt;|Ingested Content| RAG\n\n    subgraph Observability\n        Dashboards[Grafana]\n        Metrics[Prometheus]\n        Logs[Centralized Logging]\n    end\n\n    API --&gt; Metrics\n    RAG --&gt; Metrics\n    API --&gt; Logs\n    RAG --&gt; Logs</code></pre>"},{"location":"architecture/diagrams/code/","title":"Code Diagram (Retriever Module)","text":"<pre><code>%%{init: {'theme': 'forest'}}%%\nclassDiagram\n    class Retriever {\n        +load_documents()\n        +search(query)\n        +filter_results()\n    }\n\n    class Embedder {\n        +compute_embeddings()\n    }\n\n    Retriever --&gt; Embedder</code></pre>"},{"location":"architecture/diagrams/components/","title":"Components Diagram (RAG Worker)","text":"<pre><code>%%{init: {'theme': 'forest'}}%%\ngraph TD\n    Worker[RAG Worker]\n    Retriever[Document Retriever]\n    Ranker[Ranker / Scorer]\n    Embedder[Embedding Service]\n    LLM[LLM Inference]\n\n    Worker --&gt; Retriever\n    Worker --&gt; Ranker\n    Retriever --&gt; Embedder\n    Ranker --&gt; LLM</code></pre>"},{"location":"architecture/diagrams/containers/","title":"Containers Diagram","text":"<pre><code>%%{init: {'theme': 'forest'}}%%\ngraph LR\n    FPnA[FPnA Copilot System]\n    API[FastAPI Service]\n    Worker[RAG Worker]\n    VectorDB[Chroma Vector DB]\n    LLM[Azure OpenAI LLM]\n\n    FPnA --&gt; API\n    API --&gt; Worker\n    Worker --&gt; VectorDB\n    Worker --&gt; LLM</code></pre>"},{"location":"architecture/diagrams/context/","title":"Context Diagram","text":"<pre><code>%%{init: {'theme': 'forest'}}%%\ngraph TB\n    User[Bank User]\n    FPnA[FPnA Copilot System]\n    LLM[Azure OpenAI LLM]\n    DB[Snowflake Database]\n\n    User --&gt;|asks questions| FPnA\n    FPnA --&gt;|reads data| DB\n    FPnA --&gt;|sends prompts| LLM</code></pre>"},{"location":"runbooks/disaster-recovery/","title":"Disaster Recovery Runbook","text":"<p>Service: RAG Document Search AI Environment: Production Date: 2026-01-22 Owner: AI Platform Team  </p>"},{"location":"runbooks/disaster-recovery/#1-purpose","title":"1. Purpose","text":"<p>This document outlines procedures for recovering the RAG service in case of catastrophic failures, including database outages, API failures, or cloud region failures.</p>"},{"location":"runbooks/disaster-recovery/#2-recovery-strategy","title":"2. Recovery Strategy","text":"<ul> <li>Maintain daily backups of PostgreSQL database and vector embeddings.</li> <li>Use multi-region deployment for high availability if supported.</li> <li>Maintain Docker images in registry for quick redeployment.</li> <li>Keep LLM API keys and configuration stored securely.</li> </ul>"},{"location":"runbooks/disaster-recovery/#3-backup-procedures","title":"3. Backup Procedures","text":""},{"location":"runbooks/disaster-recovery/#31-postgresql-backup","title":"3.1 PostgreSQL Backup","text":"<pre><code>pg_dump -h &lt;host&gt; -U &lt;user&gt; -d &lt;database&gt; -F c -b -v -f \"/backups/rag_backup_$(date +%F).dump\"\n</code></pre> <p>Verify backup integrity</p> <p>Store copies in secure, redundant storage</p>"},{"location":"runbooks/disaster-recovery/#32-configuration-backup","title":"3.2 Configuration Backup","text":"<ul> <li>Kubernetes manifests (deployment.yaml, service.yaml)</li> <li>LLM API keys securely stored</li> <li>Env variables and secrets in Vault/Secret Manager</li> </ul>"},{"location":"runbooks/disaster-recovery/#4-recovery-steps","title":"4. Recovery Steps","text":""},{"location":"runbooks/disaster-recovery/#41-database-failure","title":"4.1 Database Failure","text":"<p>Restore latest backup:</p> <pre><code>pg_restore -h &lt;host&gt; -U &lt;user&gt; -d &lt;database&gt; \"/backups/rag_backup_latest.dump\"\n</code></pre> <p>Restart API and worker pods:</p> <pre><code>kubectl rollout restart deployment rag-api -n rag-prod\nkubectl rollout restart deployment rag-worker -n rag-prod\n</code></pre>"},{"location":"runbooks/disaster-recovery/#42-api-worker-failure","title":"4.2 API / Worker Failure","text":"<p>Scale up pods:</p> <pre><code>kubectl scale deployment rag-api --replicas=5 -n rag-prod\nkubectl scale deployment rag-worker --replicas=3 -n rag-prod\n</code></pre> <p>Monitor logs to confirm recovery:</p> <pre><code>kubectl logs -f &lt;pod_name&gt; -n rag-prod\n</code></pre>"},{"location":"runbooks/disaster-recovery/#43-llm-connectivity-issue","title":"4.3 LLM Connectivity Issue","text":"<ul> <li>Check Gemini API status</li> <li>Validate API keys</li> <li>Retry requests or switch to backup endpoints if available</li> </ul>"},{"location":"runbooks/disaster-recovery/#5-post-recovery-actions","title":"5. Post-Recovery Actions","text":"<ul> <li>Validate API health: /status and /metrics</li> <li>Run sample RAG queries to confirm correct embeddings and results</li> <li>Notify stakeholders of downtime and recovery</li> <li>Document root cause in incident management system</li> </ul>"},{"location":"runbooks/disaster-recovery/#6-contacts","title":"6. Contacts","text":"<ul> <li>DevOps: devops@example.com</li> <li>AI Platform: ai-platform@example.com</li> <li>LLM Provider Support: support@gemini.com</li> </ul>"},{"location":"runbooks/disaster-recovery/#7-references","title":"7. References","text":"<ul> <li>Production Runbook: /docs/runbooks/prod.md</li> <li>ADRs: /docs/adr/001-use-fastapi.md, /docs/adr/002-llm-provider.md, /docs/adr/003-vector-db.md</li> <li>Backup scripts: /scripts/db_backup.sh</li> </ul> <p>```</p>"},{"location":"runbooks/prod/","title":"Production Runbook","text":"<p>System: RAG Based Document Search AI Environment: Production Owners: AI Platform and DevOps Last Updated: 2026-01-22  </p>"},{"location":"runbooks/prod/#1-purpose","title":"1. Purpose","text":"<p>This runbook defines how to operate, monitor, and support the RAG AI service in production. It is written for on-call engineers, SREs, and platform teams.</p>"},{"location":"runbooks/prod/#2-system-overview","title":"2. System Overview","text":"<p>The RAG platform provides semantic search over enterprise documents using embeddings and an LLM.</p> <p>Core components:</p> <ul> <li>FastAPI Gateway: Exposes REST endpoints to clients</li> <li>RAG Worker: Generates embeddings, performs retrieval, calls LLM</li> <li>Vector Store: PostgreSQL with pgvector</li> <li>LLM Provider: Gemini</li> <li>Object Storage: Stores original documents</li> <li>Observability: Prometheus, Grafana, centralized logging</li> </ul> <p>High-level flow: 1. User submits query 2. Query is embedded 3. Vector search retrieves relevant chunks 4. Context is sent to Gemini 5. Response returned to user</p>"},{"location":"runbooks/prod/#3-service-endpoints","title":"3. Service Endpoints","text":"Endpoint Purpose <code>/query</code> Submit RAG query <code>/ingest</code> Upload documents <code>/status</code> Health check <code>/metrics</code> Prometheus metrics <code>/docs</code> OpenAPI UI <p>Health check response: <pre><code>{\n  \"api\": \"ok\",\n  \"vector_db\": \"ok\",\n  \"llm\": \"ok\",\n  \"storage\": \"ok\"\n}\n</code></pre></p>"},{"location":"runbooks/prod/#4-deployment-model","title":"4. Deployment Model","text":"<p>The service runs on Kubernetes.</p> <ul> <li>rag-api Deployment: FastAPI</li> <li>rag-worker Deployment: RAG pipeline</li> <li>postgres StatefulSet</li> <li>Secrets stored in Kubernetes Secrets or Vault</li> </ul> <p>Typical sizes:</p> <ul> <li>API: 3 replicas</li> <li>Workers: 2 to 5 replicas</li> <li>PostgreSQL: HA or managed service  </li> </ul>"},{"location":"runbooks/prod/#5-standard-operations","title":"5. Standard Operations","text":""},{"location":"runbooks/prod/#51-restart-services","title":"5.1 Restart Services","text":"<pre><code>kubectl rollout restart deployment rag-api -n rag-prod\nkubectl rollout restart deployment rag-worker -n rag-prod\n</code></pre>"},{"location":"runbooks/prod/#52-scale-for-load","title":"5.2 Scale for Load","text":"<pre><code>kubectl scale deployment rag-worker --replicas=5 -n rag-prod\n</code></pre>"},{"location":"runbooks/prod/#53-check-pods","title":"5.3 Check Pods","text":"<pre><code>kubectl get pods -n rag-prod\n</code></pre>"},{"location":"runbooks/prod/#54-view-logs","title":"5.4 View Logs","text":"<pre><code>kubectl logs -f &lt;pod-name&gt; -n rag-prod\n</code></pre>"},{"location":"runbooks/prod/#6-monitoring","title":"6. Monitoring","text":"<p>Key dashboards:</p> <ul> <li>Request volume</li> <li>Query latency</li> <li>Vector search latency</li> <li>Gemini API errors</li> <li>Token usage</li> <li>PostgreSQL CPU, disk, and query latency</li> </ul> <p>Key alerts:</p> <ul> <li>API error rate &gt; 2%</li> <li>Query latency &gt; 5s</li> <li>Embedding failures</li> <li>PostgreSQL disk &gt; 80%</li> <li>Gemini API failures</li> </ul>"},{"location":"runbooks/prod/#7-common-incidents","title":"7. Common Incidents","text":""},{"location":"runbooks/prod/#71-high-latency","title":"7.1 High Latency","text":"<p>Check:</p> <ul> <li>Worker CPU and memory</li> <li>PostgreSQL slow queries</li> <li>Vector index health</li> <li>Gemini rate limits</li> </ul> <p>Actions:</p> <ul> <li>Scale workers</li> <li>Enable query caching</li> <li>Check embedding batch sizes</li> </ul>"},{"location":"runbooks/prod/#8-data-operations","title":"8. Data Operations","text":""},{"location":"runbooks/prod/#81-verify-vector-data","title":"8.1 Verify Vector Data","text":"<pre><code>SELECT COUNT(*) FROM document_embeddings;\n</code></pre>"},{"location":"runbooks/prod/#82-sample-similarity-query","title":"8.2 Sample Similarity Query","text":"<pre><code>SELECT id, content\nFROM document_embeddings\nORDER BY embedding &lt;-&gt; '[0.01, 0.22, ...]'\nLIMIT 5;\n</code></pre>"},{"location":"runbooks/prod/#9-security-and-compliance","title":"9. Security and Compliance","text":"<ul> <li>All data encrypted in transit and at rest</li> <li>API protected via OAuth or API Gateway</li> <li>Audit logs enabled</li> <li>No raw customer data sent to LLM without redaction</li> <li>Prompt and response logging controlled via policy</li> </ul>"},{"location":"runbooks/prod/#10-scheduled-maintenance","title":"10. Scheduled Maintenance","text":"Task Frequency PostgreSQL backup Daily Index rebuild Weekly Model quality review Monthly Dependency patching Monthly Load testing Quarterly"},{"location":"runbooks/prod/#11-escalation-contacts","title":"11. Escalation Contacts","text":"<ul> <li>DevOps On-Call</li> <li>AI Platform Lead</li> <li>Database Team</li> <li>Security Operations</li> </ul>"},{"location":"runbooks/prod/#12-references","title":"12. References","text":"<ul> <li>Architecture: /docs/architecture/system-overview.md</li> <li>ADRs: /docs/adr</li> <li>API Spec: /docs/api/openapi.yaml</li> <li>DR Runbook: /docs/runbooks/disaster-recovery.md</li> </ul>"}]}